# Документация по проекту автоматического анализа мнений абитуриентов МАИ

## Описание проекта

Проект включает создание интерактивного дашборда в Yandex DataLens, отображающего статистику и анализ сообщений абитуриентов, поступающих в 8-й институт МАИ. Сообщения собираются из чата в Telegram и обрабатываются с помощью ChatGPT 3.5. Проект состоит из нескольких компонентов: парсера, обработчика данных, ML компонента и фронтенда.

## Архитектура приложения

### Компонент 1: Парсер

Парсер отвечает за сбор информации из различных источников и приведение её к единому формату.

#### Базовый класс парсера
- **Описание:** Базовый класс, от которого наследуются специализированные парсеры.
- **Функции:**
  - Сбор данных из определенного источника.
  - Преобразование данных в JSON-формат.
  - Сохранение данных в базу данных MongoDB.

#### Наследники парсера
- **Описание:** Специализированные парсеры, каждый из которых работает с определенным источником данных.
- **Функции:**
  - Получение данных из конкретного источника.
  - Преобразование данных в унифицированный формат JSON.
  - Сохранение данных в MongoDB.

### Компонент 2: Обработчик

Обработчик отвечает за подготовку данных для анализа и передачу их в ML компонент.

#### Конфигурация
- **Описание:** Конфигурации хранятся в файле `.ini` и обрабатываются с помощью библиотеки `configparser`.
- **Функции:**
  - Получение последней даты документов из конфигурационного файла.
  - Загрузка документов из MongoDB по указанной дате.
  - Получение требуемых атрибутов из конфигурации.
  - Определение настроений абитуриентов по атрибутам.
  - Маппинг данных с базой данных открытых источников.

#### Сохранение данных
- **Описание:** Проанализированные данные сохраняются в базу данных PostgreSQL.

#### Схема базы данных PostgreSQL
![Схема PostgreSQL](https://github.com/vaZZZy1/AdmissionsOfficeMAI2024/assets/108530450/aa7d725f-bc14-4d1c-a08b-e532b79d0f94)

### Компонент 3: ML часть

ML компонент отвечает за анализ текста и определение различных параметров на основе текста сообщений.

#### Пример обработки данных абитуриентов
- **Библиотеки:** `openai`, `dotenv`, `pandas`, `torch`, `transformers`
- **Функции:**
  - **request_to_GPT:** Функция для отправки запроса к API GPT-3.5.
  - **text_analysis_gpt:** Функция для анализа множества данных.
  - **compare_texts:** Функция сравнения двух текстов на совпадающий смысл.
  - **paraphrase_detector:** Функция для нахождения парафраз в колонке `pd.DataFrame`.
  - **replacing_similar_phrases:** Функция замены похожих фраз в текстовых данных.
  - **replacing_nulls:** Функция для замены нулевых/пустых значений.
  - **best_replaced:** Рекурсивная функция для нахождения лучшей замены фразы.

### Компонент 4: Фронтенд

Фронтенд отвечает за визуализацию данных в виде интерактивного дашборда.

#### Используемый инструмент
- **Yandex DataLens:** Бесплатный облачный сервис для визуализации данных.

#### Функции
- Автоматическая загрузка данных из базы данных PostgreSQL.
- Отображение данных в виде графиков, схем и других визуальных элементов.

## Инструкции по развертыванию

### Требования
- **MongoDB**
- **PostgreSQL**
- **Yandex DataLens**
- **Python библиотеки:** `openai`, `dotenv`, `pandas`, `torch`, `transformers`

### Шаги развертывания
1. **Настройка MongoDB и PostgreSQL:**
   - Создайте необходимые базы данных и таблицы.
2. **Конфигурация парсера:**
   - Настройте парсеры для каждого источника данных.
3. **Конфигурация обработчика:**
   - Создайте конфигурационный файл `.ini`.
4. **Настройка ML компонента:**
   - Установите необходимые библиотеки.
   - Настройте доступ к API GPT-3.5.
5. **Настройка Yandex DataLens:**
   - Создайте дашборд и настройте его для получения данных из PostgreSQL.

## Заключение

Этот проект предоставляет мощный инструмент для анализа и визуализации сообщений абитуриентов, что помогает в принятии обоснованных решений и улучшении коммуникации с потенциальными студентами. Документация предоставлена для упрощения понимания структуры и процесса разработки проекта. В случае необходимости дополнения информации, оставьте комментарии в соответствующих местах.

### Шаг 1: Установите Docker и Docker Compose


# Обновление пакетов и установка Docker
```
sudo apt-get update
sudo apt-get install -y docker.io
```

# Установка Docker Compose
```
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

### Шаг 2: Подготовка структуры проекта и файлов

Убедитесь, что структура вашего проекта соответствует следующей:

**Структура проекта:**

```
AdmissionsOfficeMAI2024/
│
├── db/
│   └── schemas/
│       ├── .sql files
│
├── docs/
├── ml/
│   ├── ml_pipeline.py
│   ├── system_prompt.txt
│
├── parser/
│   ├── __init__.py
│   ├── baseclass.py
│   ├── main.py
│   ├── save_to_monga.py
│   ├── tg.py
│   ├── tg_settings.env
│   └── Dockerfile
│
├── .env
├── keys.env
├── main_settings.env
├── main.py
├── requirements.txt
├── Dockerfile
├── Dockerfile-cron
├── docker-compose.yaml
├── logs/
│
├── run_main.sh
├── run_parser.sh
```

### Шаг 3: Проверка содержимого Docker и Cron файлов

Убедитесь, что файлы Dockerfile, Dockerfile-cron, run_main.sh, run_parser.sh и docker-compose.yaml присутствуют и корректно настроены в соответствии с требованиями вашего проекта.

### Шаг 4: Запуск Docker Compose

Перейдите в корневой каталог вашего проекта и выполните команду:

```
cd AdmissionsOfficeMAI2024
docker-compose up --build
```

### Резюме

Следующие действия будут выполнены:
1. Установка Docker и Docker Compose.
2. Подготовка структуры проекта.
3. Проверка содержимого Docker и Cron файлов.
4. Запуск Docker Compose для разворачивания проекта с MongoDB, PostgreSQL и Cron-задачами.

При выполнении этих шагов ваш проект будет успешно развернут на сервере.